

1. Pretrained LM (Transformer Elmo GPT Bert Ernie GPT2 XLNet)
2. Multi-Task (Neural machine translation by jointly learning to align and translate 
   => Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling
   => Slot-Gated Modeling for Joint Slot Filling and Intent Prediction)
3. zero-shot learning
4. Dialoga System
5. Zero-shOt Embeddings



---
https://github.com/HadoopIt/paper-reading-notes
https://arxiv.org/pdf/1801.05149.pdf
https://sites.cs.ucsb.edu/~william/papers/AdvNLP-NAACL2019.pdf
https://ai.googleblog.com/search/label/ACL

https://www.youtube.com/watch?v=IzHoNwlCGnE
http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://github.com/harvardnlp/annotated-transformer


https://ai.googleblog.com/
http://nlp.seas.harvard.edu/papers/


A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction
https://arxiv.org/pdf/1811.02166.pdf
