

1. Pretrained LM (Transformer Elmo GPT Bert Ernie GPT2 XLNet)
2. Multi-Task (Neural machine translation by jointly learning to align and translate 
   => Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling
   => Slot-Gated Modeling for Joint Slot Filling and Intent Prediction)
3. zero-shot learning
4. Dialoga System
5. Zero-shOt Embeddings



---
https://github.com/HadoopIt/paper-reading-notes
https://arxiv.org/pdf/1801.05149.pdf
https://sites.cs.ucsb.edu/~william/papers/AdvNLP-NAACL2019.pdf
https://ai.googleblog.com/search/label/ACL

https://www.youtube.com/watch?v=IzHoNwlCGnE
http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://github.com/harvardnlp/annotated-transformer


https://ai.googleblog.com/
http://nlp.seas.harvard.edu/papers/


A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction
https://arxiv.org/pdf/1811.02166.pdf


'A Context Free Gramma for Key Noun-Phrase Extraction from Text.pdf'
'A Domain Specific Key Phrase Extraction Framework for Email Corpuses.pdf'
'A Simple Word Trigger Method for Social Tag Suggestion.pdf'
'Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter.pdf'

KB-RL papers
